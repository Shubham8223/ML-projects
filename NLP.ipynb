{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUZKcaK5npim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf9850a-6842-489d-92c5-86fedeed387c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(b'like', 202562), (b'tast', 180725), (b'love', 163590), (b'flavor', 160354), (b'good', 159207), (b'great', 148019), (b'one', 140797), (b'use', 136407), (b'coffe', 126664), (b'tri', 125448)]\n",
            "[(b'tast', 48490), (b'like', 46947), (b'product', 38974), (b'one', 29732), (b'flavor', 28707), (b'would', 26609), (b'tri', 26151), (b'food', 25216), (b'coffe', 23010), (b'good', 21408)]\n",
            "(525812, 2938228) (525812, 2938228)\n"
          ]
        }
      ],
      "source": [
        "!pip install lxml\n",
        "!pip install beautifulsoup4\n",
        "import nltk\n",
        "import sqlite3 as sq\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer as cv\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer as tv\n",
        "import re\n",
        "from nltk.stem.snowball import SnowballStemmer as snw\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from nltk.stem.wordnet import WordNetLemmatizer as wl\n",
        "from bs4 import BeautifulSoup as bsp\n",
        "nltk.download('stopwords')\n",
        "con=sq.connect(\"/content/drive/MyDrive/database.sqlite\")\n",
        "fl=pd.read_sql_query(\"select * from Reviews where Score!=3\",con)\n",
        "fl.describe()\n",
        "sc=fl['Score'].to_numpy()\n",
        "sc=list(map(lambda x:\"positive\" if x>3 else \"negative\",sc))\n",
        "fl['Score']=sc\n",
        "fl=fl[fl['HelpfulnessNumerator']<=fl['HelpfulnessDenominator']]\n",
        "fl.drop_duplicates(subset={'Id','Score','Time'},keep='first',inplace=True)\n",
        "sw=set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "import copy\n",
        "snow=snw('english')\n",
        "def decontract(sen):\n",
        "  re.sub(r'won\\'t','will not',sen)\n",
        "  re.sub(r'\\'re','are',sen)\n",
        "  re.sub(r'\\'d','would',sen)\n",
        "  re.sub(r'\\'ll','will',sen)\n",
        "  re.sub(r'\\'t','not',sen)\n",
        "  re.sub(r'\\'ve','have',sen)\n",
        "  re.sub(r'\\'m','am',sen)\n",
        "  return sen\n",
        "\n",
        "def html(sen):\n",
        "  pat=re.compile(r'http\\S+')\n",
        "  cle=re.sub(pat,'',sen)\n",
        "  sen=bsp(cle,'lxml').get_text()\n",
        "  return sen\n",
        "def punc(sen):\n",
        "  pat1=re.compile(r'[?\\'!#\"]')\n",
        "  pat2=re.compile(r'[./\\,]')\n",
        "  sen1=re.sub(pat1,\"\",sen)\n",
        "  sen=re.sub(pat2,\" \",sen1)\n",
        "  return sen\n",
        "pos=[]\n",
        "neg=[]\n",
        "final_cle=[]\n",
        "for i,sub in enumerate(fl['Text'].to_numpy()):\n",
        "  fil=[]\n",
        "  cle=html(sub)\n",
        "  cle2=punc(cle)\n",
        "  cle2=decontract(cle2)\n",
        "  for sub2 in cle2.split():\n",
        "    if sub2.isalpha() and len(sub2)>2:\n",
        "      if sub2.lower() not in sw:\n",
        "        cle3=(snow.stem(sub2.lower())).encode('utf8')\n",
        "        fil.append(cle3)\n",
        "        if fl['Score'].to_numpy()[i]=='positive':\n",
        "          pos.append(cle3)\n",
        "        else:\n",
        "          neg.append(cle3)\n",
        "  final_cle.append(b\" \".join(fil))\n",
        "  \n",
        "pos_fre=nltk.FreqDist(pos)\n",
        "neg_fre=nltk.FreqDist(neg)\n",
        "print(pos_fre.most_common(10))\n",
        "print(neg_fre.most_common(10))\n",
        "\n",
        "fl['Text']=final_cle\n",
        "tok1=cv(ngram_range=(1,2))\n",
        "tok2=tv(ngram_range=(1,2))\n",
        "tok1.fit(fl['Text'].to_numpy())\n",
        "tok2.fit(fl['Text'].to_numpy())\n",
        "cvect=tok1.transform(fl['Text'].to_numpy())\n",
        "tfvect=tok2.transform(fl['Text'].to_numpy())\n",
        "print(cvect.shape,tfvect.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(tok1,'cvect.pkl')\n",
        "joblib.dump(tok2,'tfidfvect.pkl')\n",
        "f=tok1.get_feature_names()\n",
        "def nfeat(r1,feat,n):\n",
        "  r1=r1.toarray()[0]\n",
        "  arr1=r1.argsort()[::-1][0:n]\n",
        "  dfeat=[(feat[i],r1[i]) for i in arr1]\n",
        "  df1=pd.DataFrame(dfeat,columns=['feature','TFIDF'])\n",
        "  return (df1,dfeat)\n",
        "arg1,arg2=nfeat(tfvect[1,:],f,20)\n",
        "print(arg1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nQC60hVVq7s",
        "outputId": "38f1f132-0c98-40c0-ae02-83803e5bcbbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           feature     TFIDF\n",
            "0            jumbo  0.309064\n",
            "1      label jumbo  0.237394\n",
            "2      size unsalt  0.237394\n",
            "3       jumbo salt  0.237394\n",
            "4    intend repres  0.237394\n",
            "5     error vendor  0.237394\n",
            "6    vendor intend  0.230253\n",
            "7    product jumbo  0.230253\n",
            "8      unsalt sure  0.230253\n",
            "9       sure error  0.225187\n",
            "10   peanut actual  0.209049\n",
            "11     arriv label  0.204429\n",
            "12  repres product  0.199704\n",
            "13    actual small  0.190916\n",
            "14     salt peanut  0.174283\n",
            "15          peanut  0.170900\n",
            "16   peanut peanut  0.170159\n",
            "17          unsalt  0.139194\n",
            "18          repres  0.137509\n",
            "19           error  0.134202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier as sgdc\n",
        "clf=sgdc(max_iter=1000,tol=1e-3,eta0=0.1,alpha=0.001)\n",
        "clf.fit(cvect,fl['Score'].to_numpy())\n",
        "joblib.dump(clf,'model.pkl')\n",
        "print(clf.predict(cvect[4]),fl['Score'][4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdK8PpNdI6Sx",
        "outputId": "08d26c96-6de7-4576-af2f-ffa4def1adf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['positive'] positive\n"
          ]
        }
      ]
    }
  ]
}